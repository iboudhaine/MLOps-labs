{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0e9cb8",
   "metadata": {},
   "source": [
    "## MLFlow avec Pyspark\n",
    "\n",
    "Avant de commencer, installez Spark en local.\n",
    "\n",
    "Nous allons suivre la même démarche que nous avons utilisé dans le premier notebook.\n",
    "\n",
    "Nous allons utilisé aussi le même dataset .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04271821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db631c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1764867053053'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.hadoop.fs.s3a.vectored.read.min.seek.size', '128K'),\n",
       " ('spark.driver.port', '36261'),\n",
       " ('spark.driver.host', 'localhost'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.sql.artifact.isolation.enabled', 'false'),\n",
       " ('spark.app.id', 'local-1764867054005'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.app.submitTime', '1764867052749'),\n",
       " ('spark.hadoop.fs.s3a.vectored.read.max.merged.size', '2M'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/iboud/UM6P-CC/S9/cloud_computing/MLOps-labs/Lab3-MLflow/spark-warehouse'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"SPARK_LOCAL_IP\"]='127.0.0.1'\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876c6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark: 4.0.1\n",
      "matplotlib: 3.10.7\n",
      "seaborn: 0.13.2\n",
      "sklearn: 1.7.2\n",
      "mlflow: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"pyspark: {}\".format(pyspark.__version__))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"seaborn: {}\".format(sns.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"mlflow: {}\".format(mlflow.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/creditcard.csv'\n",
    "df = spark.read.csv(data_path, header = True,\n",
    "inferSchema = True)\n",
    "labelColumn = \"Class\"\n",
    "columns = df.columns\n",
    "numericCols = columns\n",
    "numericCols.remove(\"Time\")\n",
    "numericCols.remove(labelColumn)\n",
    "print(numericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58c450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0be770",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "assemblerInputs = numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "dfFeatures = df.select(F.col(labelColumn).alias('label'),*numericCols )\n",
    "normal = dfFeatures.filter(\"Class == 0\").sample(withReplacement=False, fraction=0.5, seed=2020)\n",
    "anomaly = dfFeatures.filter(\"Class == 1\")\n",
    "normal_train, normal_test = normal.randomSplit([0.8, 0.2],seed = 2020)\n",
    "anomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2],seed = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatures.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = normal_train.union(anomaly_train)\n",
    "test_set = normal_test.union(anomaly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75009c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(dfFeatures)\n",
    "train_set = pipelineModel.transform(train_set)\n",
    "test_set = pipelineModel.transform(test_set)\n",
    "selectedCols = ['label', 'features'] + numericCols\n",
    "train_set = train_set.select(selectedCols)\n",
    "test_set = test_set.select(selectedCols)\n",
    "print(\"Training Dataset Count: \", train_set.count())\n",
    "print(\"Test Dataset Count: \", test_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd590f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(spark_model, train_set):\n",
    "    trained_model = spark_model.fit(train_set)\n",
    "    trainingSummary = trained_model.summary\n",
    "    pyspark_auc_score = trainingSummary.areaUnderROC\n",
    "    mlflow.log_metric(\"train_acc\", trainingSummary.accuracy)\n",
    "    mlflow.log_metric(\"train_AUC\", pyspark_auc_score)\n",
    "    print(\"Training Accuracy: \", trainingSummary.accuracy)\n",
    "    print(\"Training AUC:\", pyspark_auc_score)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c79071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(spark_model, test_set):\n",
    "    evaluation_summary = spark_model.evaluate(test_set)\n",
    "    eval_acc = evaluation_summary.accuracy\n",
    "    eval_AUC = evaluation_summary.areaUnderROC\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"eval_AUC\", eval_AUC)\n",
    "    print(\"Evaluation Accuracy: \", eval_acc)\n",
    "    print(\"Evaluation AUC: \", eval_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b23638",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol ='label', maxIter=10)\n",
    "mlflow.set_experiment(\"PySpark_CreditCard\")\n",
    "with mlflow.start_run():\n",
    "    trainedLR = train(lr, train_set)\n",
    "    evaluate(trainedLR, test_set)\n",
    "    mlflow.spark.log_model(trainedLR,\"creditcard_model_pyspark\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588b788",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "copiez l'ID run a partir de l'interface MLFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.spark.load_model(\"runs:/votre_RUN_ID/creditcard_model_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_set)\n",
    "y_true = predictions.select(['label']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78915eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUC Score: {roc_auc_score(y_true, y_pred):.3%}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax = sns.heatmap(conf_matrix, annot=True,fmt='g')\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c48d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb8787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
